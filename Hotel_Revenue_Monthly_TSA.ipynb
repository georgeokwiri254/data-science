{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Hotel Revenue MONTHLY Time Series Analysis - EDA & Preprocessing\n## Multi-Year Forecasting for 2025 Annual Close\n\n**Project Goal:** Predict hotel revenue metrics for September - December 2025 using historical **MONTHLY data from 2009-2025**.\n\n**Current Date:** August 2025\n\n**Data Splits (MONTHLY):**\n- Training: 2009-01 to 2025-05 (16+ years of history!)\n- Validation: 2025-06 to 2025-08 (3 months)\n- Test/Forecast: 2025-09 to 2025-12 (4 months)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Data Loading\n",
    "\n",
    "### 1.1 Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistical libraries for time series\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define key constants\nCURRENT_DATE = '2025-08'  # MONTHLY format\nTARGET_VARIABLES = ['RevPar', 'ADR', 'Revenue', 'Occupancy_Pct']\n\n# Data split dates (MONTHLY FORMAT)\nTRAIN_END = '2025-05'  # May 2025\nVALIDATION_START = '2025-06'  # June 2025\nVALIDATION_END = '2025-08'  # August 2025\nFORECAST_START = '2025-09'  # September 2025\nFORECAST_END = '2025-12'  # December 2025\n\n# Color palette for visualizations\nCOLORS = sns.color_palette('husl', 8)\n\nprint(f\"Configuration set:\")\nprint(f\"  - Current Date: {CURRENT_DATE}\")\nprint(f\"  - Training Period: 2009-01 to {TRAIN_END}\")\nprint(f\"  - Validation Period: {VALIDATION_START} to {VALIDATION_END}\")\nprint(f\"  - Forecast Period: {FORECAST_START} to {FORECAST_END}\")\nprint(f\"  - Target Variables: {TARGET_VARIABLES}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Data Ingestion (Google Colab File Upload)\n\n**Instructions:** Upload your `monthly_revenue_2009_2025_all.csv` file when prompted below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Google Colab file upload\nfrom google.colab import files\n\nprint(\"Please upload your monthly_revenue_2009_2025_all.csv file:\")\nuploaded = files.upload()\n\n# Get the uploaded filename\nfilename = list(uploaded.keys())[0]\nprint(f\"\\nFile '{filename}' uploaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the data\ndf = pd.read_csv(filename)\n\n# Create Date column from Year and Month columns\ndf['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str).str.zfill(2) + '-01')\n\n# Rename columns to match notebook expectations\n# CSV columns: Year,Month,Month_Name,Days,Total_Revenue,Total_Rooms_Sold,ADR,Occupancy_Pct,RevPar\n# Expected: Date, Revenue, ADR, RevPar, Occupancy_Pct\ndf = df.rename(columns={\n    'Total_Revenue': 'Revenue',\n    'Total_Rooms_Sold': 'Rooms_Sold'\n})\n\n# Sort by date\ndf = df.sort_values('Date').reset_index(drop=True)\n\nprint(f\"Data loaded successfully!\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\nprint(f\"Columns: {list(df.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 2.1 Initial Data Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of records: {len(df)}\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names:\\n{list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TYPES AND MEMORY\")\n",
    "print(\"=\"*80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First and last records\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIRST 5 RECORDS\")\n",
    "print(\"=\"*80)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAST 5 RECORDS\")\n",
    "print(\"=\"*80)\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\"*80)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing Count': missing.values,\n",
    "    'Percentage': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DUPLICATE CHECK\")\n",
    "print(\"=\"*80)\n",
    "duplicates = df.duplicated(subset='Date').sum()\n",
    "print(f\"Number of duplicate dates: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate dates:\")\n",
    "    display(df[df.duplicated(subset='Date', keep=False)].sort_values('Date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot all target variables over time\nfig, axes = plt.subplots(4, 1, figsize=(16, 12))\n\nfor idx, var in enumerate(TARGET_VARIABLES):\n    axes[idx].plot(df['Date'], df[var], linewidth=1, color=COLORS[idx])\n    axes[idx].set_title(f'{var} Over Time (2009-2025)', fontsize=14, fontweight='bold')\n    axes[idx].set_xlabel('Date', fontsize=11)\n    axes[idx].set_ylabel(var, fontsize=11)\n    axes[idx].grid(True, alpha=0.3)\n    \n    # Add vertical lines for data splits\n    axes[idx].axvline(x=pd.to_datetime(TRAIN_END + '-01'), color='blue', linestyle='--', \n                      linewidth=1.5, label='Train End', alpha=0.7)\n    axes[idx].axvline(x=pd.to_datetime(VALIDATION_END + '-01'), color='red', linestyle='--', \n                      linewidth=1.5, label='Current Date', alpha=0.7)\n    axes[idx].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Seasonality Analysis - Dubai Tourism Seasons\n\n**Note:** Day-of-week analysis not applicable for monthly data. Analyzing monthly patterns instead."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Day-of-week analysis REMOVED - not applicable for monthly data\n# This cell intentionally left blank for monthly analysis\nprint(\"✓ Day-of-week analysis skipped (not applicable for monthly granularity)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Box plots by Month (across all years - showing Dubai seasonality)\ndf['Month'] = df['Date'].dt.month\nmonth_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes = axes.ravel()\n\n# Filter to only use data up to current date for analysis\ndf_for_analysis = df[df['Date'] <= pd.to_datetime(CURRENT_DATE + '-01')]\n\nfor idx, var in enumerate(TARGET_VARIABLES):\n    sns.boxplot(data=df_for_analysis, x='Month', y=var, \n                palette='viridis', ax=axes[idx])\n    axes[idx].set_title(f'{var} Distribution by Month (2009-2025)', fontsize=14, fontweight='bold')\n    axes[idx].set_xlabel('Month', fontsize=11)\n    axes[idx].set_ylabel(var, fontsize=11)\n    axes[idx].set_xticklabels(month_names)\n    \n    # Highlight Dubai seasons with background shading\n    # High Season: Oct-Apr (months 10,11,12,1,2,3,4)\n    # Low Season: May-Sep (months 5,6,7,8,9)\n    axes[idx].axvspan(-0.5, 3.5, alpha=0.1, color='green', label='High Season (Oct-Apr)')\n    axes[idx].axvspan(9.5, 11.5, alpha=0.1, color='green')\n    axes[idx].axvspan(4.5, 8.5, alpha=0.1, color='red', label='Low Season (May-Sep)')\n    axes[idx].legend(loc='upper right', fontsize=9)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Time Series Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Decompose RevPar time series (only training data)\ntrain_eda = df[df['Date'] <= TRAIN_END].copy().set_index('Date')\n\ndecomposition = seasonal_decompose(train_eda['RevPar'], model='additive', period=12)\n\nfig, axes = plt.subplots(4, 1, figsize=(16, 12))\n\ndecomposition.observed.plot(ax=axes[0], color='blue', marker='o')\naxes[0].set_ylabel('Observed', fontsize=11)\naxes[0].set_title('Time Series Decomposition - RevPar MONTHLY (Training Data)', fontsize=14, fontweight='bold')\n\ndecomposition.trend.plot(ax=axes[1], color='green', marker='o')\naxes[1].set_ylabel('Trend', fontsize=11)\n\ndecomposition.seasonal.plot(ax=axes[2], color='orange')\naxes[2].set_ylabel('Seasonal (12-month)', fontsize=11)\n\ndecomposition.resid.plot(ax=axes[3], color='red', marker='o')\naxes[3].set_ylabel('Residual', fontsize=11)\naxes[3].set_xlabel('Date', fontsize=11)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ACF and PACF for RevPar\nfig, axes = plt.subplots(2, 1, figsize=(16, 8))\n\nplot_acf(train_eda['RevPar'].dropna(), lags=24, ax=axes[0])\naxes[0].set_title('Autocorrelation Function (ACF) - RevPar MONTHLY', fontsize=14, fontweight='bold')\n\nplot_pacf(train_eda['RevPar'].dropna(), lags=24, ax=axes[1])\naxes[1].set_title('Partial Autocorrelation Function (PACF) - RevPar MONTHLY', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Stationarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dickey-Fuller Test\n",
    "def adf_test(series, name=''):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f\"\\nAugmented Dickey-Fuller Test - {name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ADF Statistic: {result[0]:.6f}\")\n",
    "    print(f\"p-value: {result[1]:.6f}\")\n",
    "    print(f\"Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"\\nResult: STATIONARY (p-value = {result[1]:.6f})\")\n",
    "    else:\n",
    "        print(f\"\\nResult: NON-STATIONARY (p-value = {result[1]:.6f})\")\n",
    "\n",
    "for var in TARGET_VARIABLES:\n",
    "    adf_test(train_eda[var], name=var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Feature Engineering and Preprocessing\n",
    "\n",
    "Transform raw time series data into ML-ready features.\n",
    "\n",
    "### 3.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create clean copy\ndf_clean = df.copy()\n\nprint(\"=\"*80)\nprint(\"DATA CLEANING (MONTHLY)\")\nprint(\"=\"*80)\n\n# Handle missing values using interpolation (monthly)\nnumeric_cols = ['Revenue', 'ADR', 'RevPar', 'Occupancy_Pct']\nmissing_before = df_clean[numeric_cols].isnull().sum().sum()\n\nfor col in numeric_cols:\n    if df_clean[col].isnull().sum() > 0:\n        df_clean[col] = df_clean[col].interpolate(method='linear')  # Linear interpolation for monthly gaps\n        # Fill any remaining NaN at the start/end with forward/backward fill\n        df_clean[col] = df_clean[col].fillna(method='ffill').fillna(method='bfill')\n\nmissing_after = df_clean[numeric_cols].isnull().sum().sum()\nprint(f\"Missing values before: {missing_before}\")\nprint(f\"Missing values after: {missing_after}\")\nprint(\"\\n✓ Data cleaning complete (monthly interpolation used)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"TEMPORAL FEATURES (MONTHLY - OPTIMIZED, NO QUARTER)\")\nprint(\"=\"*80)\n\n# Create MINIMAL temporal features for monthly data\ndf_clean['Month'] = df_clean['Date'].dt.month  # 1-12 (for reference only)\n\n# Dubai Hotel Seasonality (CRITICAL FEATURES - NO QUARTER!)\ndf_clean['High_Season'] = df_clean['Month'].isin([10,11,12,1,2,3,4]).astype(int)  # Oct-Apr (high tourism)\ndf_clean['Low_Season'] = df_clean['Month'].isin([5,6,7,8,9]).astype(int)  # May-Sep (low tourism)\n\nprint(\"✓ Created 2 temporal features (OPTIMIZED - NO QUARTER):\")\nprint(\"  - High_Season (Oct-Apr) - Dubai tourism high season\")\nprint(\"  - Low_Season (May-Sep) - Dubai tourism low season\")\nprint(\"\\n✓ REMOVED: Quarter (redundant with High/Low Season)\")\nprint(\"  Total: 2 features (minimized for ~200 monthly records)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"LAGGED FEATURES (MONTHLY)\")\nprint(\"=\"*80)\n\nlag_features = ['RevPar', 'ADR', 'Occupancy_Pct', 'Revenue']\nlag_periods = [1]  # 1-MONTH lag (not 1-day!)\n\nfor feature in lag_features:\n    for lag in lag_periods:\n        df_clean[f'{feature}_lag_{lag}'] = df_clean[feature].shift(lag)\n\nprint(f\"✓ Created {len(lag_features) * len(lag_periods)} lagged features\")\nprint(f\"  - Variables: {lag_features}\")\nprint(f\"  - Lags: {lag_periods} MONTH (1-month lag)\")\nprint(f\"  - Note: Using only 1-month lag to avoid overfitting (~200 monthly records)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Moving Averages and Rolling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"ROLLING STATISTICS\")\nprint(\"=\"*80)\n\n# IMPROVED: Remove MA/Rolling features for XGBoost to reduce multicollinearity\n# Tree-based models like XGBoost automatically capture these patterns\n# Keep these features commented out in case you use LSTM or SARIMA\n\nprint(\"✓ Skipping rolling features (MA, Std Dev) for XGBoost\")\nprint(\"  - Reason: XGBoost captures these patterns automatically\")\nprint(\"  - Reduces multicollinearity with lag features\")\nprint(\"  - If using LSTM/SARIMA, uncomment the code below:\")\nprint()\n\n# Uncomment below if using LSTM or SARIMA\n\"\"\"\nwindows = [7, 14, 30]\n\nfor feature in lag_features:\n    for window in windows:\n        df_clean[f'{feature}_ma_{window}'] = df_clean[feature].rolling(\n            window=window, min_periods=1).mean()\n        df_clean[f'{feature}_std_{window}'] = df_clean[feature].rolling(\n            window=window, min_periods=1).std()\n\nprint(f\"✓ Created {len(lag_features) * len(windows) * 2} rolling features\")\nprint(f\"  - Statistics: Mean, Std Dev\")\nprint(f\"  - Windows: {windows} days\")\n\"\"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"SEASONAL ENCODING - DUBAI PATTERN (NO MONTH DUMMIES, NO QUARTER)\")\nprint(\"=\"*80)\n\n# HIGH_SEASON and LOW_SEASON already created in Cell 29\n# NO month dummies (redundant with High_Season/Low_Season)\n# NO day-of-week encoding (not applicable for monthly data)\n# NO Quarter (redundant with High_Season/Low_Season)\n\nprint(\"✓ Using Dubai Seasonality features (already created):\")\nprint(\"  - High_Season: Oct-Apr (10,11,12,1,2,3,4) = 1\")\nprint(\"  - Low_Season: May-Sep (5,6,7,8,9) = 1\")\nprint(\"\\n✓ SKIPPED: Month dummies (would add 12 redundant features)\")\nprint(\"✓ SKIPPED: Day-of-week encoding (not applicable for monthly)\")\nprint(\"✓ SKIPPED: Quarter (redundant - Q2 splits Dubai seasons)\")\n\n# Convert boolean to int (if any)\nbool_cols = [col for col in df_clean.columns if df_clean[col].dtype == 'bool']\nfor col in bool_cols:\n    df_clean[col] = df_clean[col].astype(int)\n\nprint(f\"\\nTotal temporal features: 2 (High_Season, Low_Season) - NO QUARTER!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Chronological Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"DATA SPLITTING (MONTHLY)\")\nprint(\"=\"*80)\n\n# Use FULL history (2009-2025) - monthly data benefits from longer history!\nprint(\"\\n✅ Using FULL historical data (2009-2025) for monthly analysis\")\nprint(\"  - Rationale: Monthly data less noisy than daily, more stable patterns\")\nprint(f\"Full dataset: {len(df_clean)} monthly records from {df_clean['Date'].min()} to {df_clean['Date'].max()}\")\n\n# Create splits - convert YYYY-MM strings to datetime for comparison\ntrain_end_date = pd.to_datetime(TRAIN_END + '-01')\nval_start_date = pd.to_datetime(VALIDATION_START + '-01')\nval_end_date = pd.to_datetime(VALIDATION_END + '-01')\nforecast_start_date = pd.to_datetime(FORECAST_START + '-01')\nforecast_end_date = pd.to_datetime(FORECAST_END + '-01')\n\ntrain_data = df_clean[df_clean['Date'] <= train_end_date].copy()\nvalidation_data = df_clean[(df_clean['Date'] >= val_start_date) & \n                           (df_clean['Date'] <= val_end_date)].copy()\ntest_data = df_clean[(df_clean['Date'] >= forecast_start_date) & \n                     (df_clean['Date'] <= forecast_end_date)].copy()\n\nprint(f\"\\nTraining Set:\")\nprint(f\"  Period: {train_data['Date'].min()} to {train_data['Date'].max()}\")\nprint(f\"  Records: {len(train_data)} months\")\n\nprint(f\"\\nValidation Set:\")\nprint(f\"  Period: {validation_data['Date'].min()} to {validation_data['Date'].max()}\")\nprint(f\"  Records: {len(validation_data)} months\")\n\nprint(f\"\\nTest/Forecast Set:\")\nprint(f\"  Period: {test_data['Date'].min()} to {test_data['Date'].max()}\")\nprint(f\"  Records: {len(test_data)} months\")\n\nprint(f\"\\nTotal: {len(df_clean)} monthly records\")\nprint(f\"✓ Sample-to-feature ratio (after removing NaN): ~{len(train_data)//6 if len(train_data) > 0 else 0}:1 (target: >10:1)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"FEATURE SCALING (STANDARDIZATION - MONTHLY, NO QUARTER)\")\nprint(\"=\"*80)\n\n# Columns NOT to scale (binary/categorical only - NO QUARTER!)\ncols_not_to_scale = ['Date', 'Month', 'High_Season', 'Low_Season']\n\n# Get numeric columns to scale (targets + lags only)\nnumeric_cols_all = df_clean.select_dtypes(include=[np.number]).columns.tolist()\ncols_to_scale = [col for col in numeric_cols_all if col not in cols_not_to_scale]\n\nprint(f\"Columns NOT scaled: {len(cols_not_to_scale)} (categorical/binary)\")\nprint(f\"  {cols_not_to_scale}\")\nprint(f\"\\nColumns TO scale: {len(cols_to_scale)} (continuous values)\")\nprint(f\"  {cols_to_scale}\")\n\n# Save ORIGINAL data for SARIMAX (statistical models use unscaled data)\ntrain_data_original = train_data.copy()\nvalidation_data_original = validation_data.copy()\ntest_data_original = test_data.copy()\n\n# Fit scaler ONLY on training data\nscaler = StandardScaler()\nscaler.fit(train_data[cols_to_scale])\n\n# Transform all datasets\ntrain_data[cols_to_scale] = scaler.transform(train_data[cols_to_scale])\nvalidation_data[cols_to_scale] = scaler.transform(validation_data[cols_to_scale])\ntest_data[cols_to_scale] = scaler.transform(test_data[cols_to_scale])\n\nprint(\"\\n✓ StandardScaler fitted on training data ONLY\")\nprint(\"✓ Applied to train, validation, and test sets\")\nprint(\"✓ ORIGINAL data saved for SARIMAX model (train_data_original, etc.)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Create ML-Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"ML-READY DATASET\")\nprint(\"=\"*80)\n\n# Combine all data\nml_ready_data = pd.concat([train_data, validation_data, test_data], ignore_index=True)\n\n# Add dataset identifier\nml_ready_data['Dataset'] = 'Train'\nml_ready_data.loc[ml_ready_data['Date'] >= VALIDATION_START, 'Dataset'] = 'Validation'\nml_ready_data.loc[ml_ready_data['Date'] >= FORECAST_START, 'Dataset'] = 'Test'\n\n# Drop redundant columns before export\nredundant_cols = ['DOW', 'DayOfWeek_Name', 'Rm Sold', 'Year', 'Month_Name', 'Days', 'Total_Revenue', 'Total_Rooms_Sold']\ncols_to_drop = [col for col in redundant_cols if col in ml_ready_data.columns]\nif cols_to_drop:\n    ml_ready_data = ml_ready_data.drop(columns=cols_to_drop)\n    print(f\"\\n✓ Removed {len(cols_to_drop)} redundant columns: {cols_to_drop}\")\n\nprint(f\"\\nTotal records: {len(ml_ready_data)}\")\nprint(f\"Total features: {len(ml_ready_data.columns)}\")\nprint(f\"Date range: {ml_ready_data['Date'].min()} to {ml_ready_data['Date'].max()}\")\n\nprint(f\"\\nDataset Distribution:\")\nprint(ml_ready_data['Dataset'].value_counts().sort_index())\n\nprint(\"\\nSample of ML-Ready Data:\")\n# Show features that actually exist for monthly data\ndisplay_cols = ['Date', 'RevPar', 'High_Season', 'Low_Season', 'Dataset']\n# Add lag feature if it exists\nif 'RevPar_lag_1' in ml_ready_data.columns:\n    display_cols.insert(2, 'RevPar_lag_1')\ndisplay(ml_ready_data[display_cols].head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Export ML-Ready Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export to CSV\noutput_filename = 'combined_occupancy_ml_ready.csv'\nml_ready_data.to_csv(output_filename, index=False)\n\nprint(\"=\"*80)\nprint(\"EXPORT COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\\n✓ ML-ready data saved: {output_filename}\")\nprint(f\"✓ Ready for model training and forecasting\")\n\n# Download\nfiles.download(output_filename)\nprint(f\"\\n✓ File downloaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Preprocessing Summary\n\n**Completed Steps:**\n1. ✓ Data cleaning and missing value imputation\n2. ✓ Temporal features (Day of Week, Month, Quarter, etc.)\n3. ✓ Lagged variables (1-day lag only)\n4. ✓ Moving averages: REMOVED (not needed for XGBoost)\n5. ✓ One-hot encoding (DOW, Month)\n6. ✓ Chronological data splitting (Train/Validation/Test)\n7. ✓ Feature scaling (StandardScaler fitted on training only)\n8. ✓ Export ML-ready dataset\n\n**Next Steps:**\n- Model training (XGBoost, SARIMA, LSTM)\n- Model evaluation on validation set\n- Final forecast: September - December 2025"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Model Training and Selection\n",
    "\n",
    "Train top-ranked models using the prepared training data.\n",
    "\n",
    "### 4.0 Load Preprocessed ML-Ready Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# If you already have ml_ready_data from Section 3, skip this cell\n# Otherwise, upload the preprocessed file\n\nprint(\"Upload combined_occupancy_ml_ready.csv if not already in memory:\")\ntry:\n    # Check if ml_ready_data exists\n    print(f\"ML-ready data already loaded: {ml_ready_data.shape}\")\nexcept NameError:\n    # Upload and load the preprocessed file\n    uploaded_ml = files.upload()\n    ml_filename = list(uploaded_ml.keys())[0]\n    ml_ready_data = pd.read_csv(ml_filename)\n    ml_ready_data['Date'] = pd.to_datetime(ml_ready_data['Date'])\n    print(f\"ML-ready data loaded: {ml_ready_data.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare datasets for modeling\nprint(\"=\"*80)\nprint(\"PREPARING DATA FOR MODELING (MONTHLY - 6 FEATURES, NO QUARTER)\")\nprint(\"=\"*80)\n\n# Split by Dataset identifier\ntrain_df = ml_ready_data[ml_ready_data['Dataset'] == 'Train'].copy()\nval_df = ml_ready_data[ml_ready_data['Dataset'] == 'Validation'].copy()\ntest_df = ml_ready_data[ml_ready_data['Dataset'] == 'Test'].copy()\n\nprint(f\"\\nTraining Set: {len(train_df)} months\")\nprint(f\"Validation Set: {len(val_df)} months\")\nprint(f\"Test Set: {len(test_df)} months\")\n\n# Define feature columns (6 features - NO QUARTER!)\nexclude_cols = ['Date', 'Month', 'Dataset'] + TARGET_VARIABLES\nfeature_cols = [col for col in ml_ready_data.columns if col not in exclude_cols]\n\n# Remove any columns with NaN (from lagging)\ntrain_df_clean = train_df.dropna()\nval_df_clean = val_df.dropna()\n\nprint(f\"\\nFeature columns: {len(feature_cols)}\")\nprint(f\"  {feature_cols}\")\nprint(f\"\\nExpected: 6 features [High_Season, Low_Season, RevPar_lag_1, ADR_lag_1, Revenue_lag_1, Occupancy_Pct_lag_1]\")\n\nprint(f\"\\nTraining set after removing NaN: {len(train_df_clean)} months\")\nprint(f\"Validation set after removing NaN: {len(val_df_clean)} months\")\nprint(f\"\\nSample-to-feature ratio: {len(train_df_clean)}:{len(feature_cols)} = {len(train_df_clean)//len(feature_cols) if len(feature_cols) > 0 else 0}:1\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.0.1 Important Note: Data Scaling Strategy\n\n**Different models require different data scales:**\n\n1. **SARIMA (Statistical Model)**\n   - Trains on **ORIGINAL (unscaled)** data\n   - Statistical models expect data in its natural scale\n   - Predictions are directly in AED (currency units)\n   - Better performance when working with actual values\n\n2. **XGBoost (Tree-based Model)**\n   - Trains on **SCALED** data\n   - Benefits from standardization for regularization\n   - Predictions need inverse transformation back to AED\n\n3. **LSTM (Neural Network)**\n   - Trains on **SCALED** data  \n   - Neural networks require normalized inputs for optimization\n   - Predictions need inverse transformation back to AED\n\n**Why this matters:**\n- SARIMA was performing poorly when trained on scaled data\n- Each model type has different requirements for optimal performance\n- We maintain separate datasets to ensure each model gets the appropriate scale",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline Model: SARIMA\n",
    "\n",
    "Establish a performance benchmark using SARIMA (Seasonal ARIMA)."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Import SARIMAX libraries\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nprint(\"=\"*80)\nprint(\"BASELINE MODEL: SARIMAX (WITH DUBAI SEASONALITY)\")\nprint(\"=\"*80)\n\n# SARIMAX uses ORIGINAL (unscaled) data + exogenous variables\ntrain_sarimax = train_data_original.set_index('Date')['RevPar']\n# Ensure High_Season and Low_Season exist in the training data\nif 'High_Season' not in train_data_original.columns:\n    train_data_original['High_Season'] = train_data_original['Month'].isin([10,11,12,1,2,3,4]).astype(int)\n    train_data_original['Low_Season'] = train_data_original['Month'].isin([5,6,7,8,9]).astype(int)\n    \ntrain_exog = train_data_original.set_index('Date')[['High_Season', 'Low_Season']]\n\nval_sarimax = validation_data_original.set_index('Date')['RevPar']\nif 'High_Season' not in validation_data_original.columns:\n    validation_data_original['High_Season'] = validation_data_original['Month'].isin([10,11,12,1,2,3,4]).astype(int)\n    validation_data_original['Low_Season'] = validation_data_original['Month'].isin([5,6,7,8,9]).astype(int)\n    \nval_exog = validation_data_original.set_index('Date')[['High_Season', 'Low_Season']]\n\nprint(f\"\\nSARIMAX Training Data (Original Scale):\")\nprint(f\"  Records: {len(train_sarimax)} months\")\nprint(f\"  RevPar range: AED {train_sarimax.min():.2f} - {train_sarimax.max():.2f}\")\nprint(f\"  Mean: AED {train_sarimax.mean():.2f}\")\nprint(f\"  Exogenous vars: High_Season, Low_Season (Dubai pattern)\")\n\n# Train SARIMAX model with Dubai seasonality\nsarimax_model = SARIMAX(\n    train_sarimax,\n    exog=train_exog,  # EXOGENOUS VARIABLES!\n    order=(1, 1, 1),\n    seasonal_order=(1, 1, 1, 12),  # s=12 for monthly!\n    enforce_stationarity=False,\n    enforce_invertibility=False\n)\n\nprint(\"\\nTraining SARIMAX with Dubai seasonality exogenous variables...\")\nsarimax_fit = sarimax_model.fit(disp=False, maxiter=200)\nprint(\"✓ SARIMAX model trained on ORIGINAL (unscaled) data\")\n\n# Predict with exogenous variables\nsarimax_val_pred = sarimax_fit.forecast(steps=len(val_sarimax), exog=val_exog.values)\n\n# Calculate metrics\nsarimax_rmse = np.sqrt(mean_squared_error(val_sarimax, sarimax_val_pred))\nsarimax_mae = mean_absolute_error(val_sarimax, sarimax_val_pred)\nsarimax_r2 = r2_score(val_sarimax, sarimax_val_pred)\nsarimax_mape = np.mean(np.abs((val_sarimax - sarimax_val_pred) / val_sarimax)) * 100\n\nprint(f\"\\nSARIMAX Validation Performance (Original Scale):\")\nprint(f\"  RMSE: AED {sarimax_rmse:.2f}\")\nprint(f\"  MAE: AED {sarimax_mae:.2f}\")\nprint(f\"  R²: {sarimax_r2:.4f}\")\nprint(f\"  MAPE: {sarimax_mape:.2f}%\")\n\nprint(f\"\\n✓ SARIMAX predictions are in ORIGINAL SCALE (AED)\")\nprint(f\"  Prediction range: AED {sarimax_val_pred.min():.2f} - {sarimax_val_pred.max():.2f}\")\n\n# Store results\nresults = {\n    'SARIMAX': {'RMSE': sarimax_rmse, 'MAE': sarimax_mae, 'R2': sarimax_r2, 'MAPE': sarimax_mape}\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# This cell has been removed - duplicate SARIMA cell\n# SARIMAX with Dubai seasonality is implemented in Cell 50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Advanced Model: XGBoost Regressor\n",
    "\n",
    "Train XGBoost for multi-output regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import XGBoost\nimport xgboost as xgb\nfrom sklearn.multioutput import MultiOutputRegressor\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ADVANCED MODEL: XGBOOST REGRESSOR (MONTHLY - REDUCED COMPLEXITY)\")\nprint(\"=\"*80)\n\n# Prepare data\nX_train = train_df_clean[feature_cols]\ny_train = train_df_clean[TARGET_VARIABLES]\n\nX_val = val_df_clean[feature_cols]\ny_val = val_df_clean[TARGET_VARIABLES]\n\nprint(f\"\\nTraining features shape: {X_train.shape}\")\nprint(f\"Training targets shape: {y_train.shape}\")\n\n# MONTHLY: Reduced complexity for ~200 monthly records\nxgb_model = xgb.XGBRegressor(\n    n_estimators=100,  # REDUCED from 200 for monthly\n    max_depth=4,  # REDUCED from 5 for monthly\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.7,\n    reg_alpha=0.1,  # L1 regularization\n    reg_lambda=1.0,  # L2 regularization\n    min_child_weight=3,  # Prevents overfitting\n    random_state=42,\n    n_jobs=-1\n)\n\n# Use MultiOutputRegressor for multiple targets\nmulti_xgb = MultiOutputRegressor(xgb_model)\n\nprint(\"\\nTraining XGBoost model (monthly-optimized complexity)...\")\nmulti_xgb.fit(X_train, y_train)\nprint(\"✓ XGBoost model trained\")\n\n# Predict on validation set\nxgb_val_pred = multi_xgb.predict(X_val)\n\n# FIXED: Inverse transform predictions and actuals to original scale\nxgb_val_pred_original = xgb_val_pred.copy()\ny_val_original = y_val.copy()\n\n# Inverse transform each target column\nfor idx_col, target in enumerate(TARGET_VARIABLES):\n    if target in cols_to_scale:\n        # Get the scaler index for this target\n        target_idx = cols_to_scale.index(target)\n        \n        # Inverse transform predictions\n        dummy_pred = np.zeros((len(xgb_val_pred), len(cols_to_scale)))\n        dummy_pred[:, target_idx] = xgb_val_pred[:, idx_col]\n        inv_transformed_pred = scaler.inverse_transform(dummy_pred)\n        xgb_val_pred_original[:, idx_col] = inv_transformed_pred[:, target_idx]\n        \n        # Inverse transform actuals\n        dummy_actual = np.zeros((len(y_val), len(cols_to_scale)))\n        dummy_actual[:, target_idx] = y_val.iloc[:, idx_col]\n        inv_transformed_actual = scaler.inverse_transform(dummy_actual)\n        y_val_original.iloc[:, idx_col] = inv_transformed_actual[:, target_idx]\n\n# Calculate metrics for each target (on scaled data for consistency)\nprint(f\"\\nXGBoost Validation Performance (Scaled Data):\")\nxgb_metrics = {}\nfor idx, target in enumerate(TARGET_VARIABLES):\n    rmse = np.sqrt(mean_squared_error(y_val.iloc[:, idx], xgb_val_pred[:, idx]))\n    mae = mean_absolute_error(y_val.iloc[:, idx], xgb_val_pred[:, idx])\n    r2 = r2_score(y_val.iloc[:, idx], xgb_val_pred[:, idx])\n    \n    xgb_metrics[target] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n    print(f\"\\n  {target}:\")\n    print(f\"    RMSE: {rmse:.2f}\")\n    print(f\"    MAE: {mae:.2f}\")\n    print(f\"    R²: {r2:.4f}\")\n\nresults['XGBoost'] = xgb_metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Validation Set: Actual vs Predicted Comparison\n",
    "\n",
    "Compare predictions against actual values (June-August 2025) in original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET: ACTUAL VS PREDICTED (ORIGINAL SCALE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Recalculate metrics on original scale\n",
    "print(f\"\\nXGBoost Validation Performance (Original Scale):\")\n",
    "xgb_metrics_original = {}\n",
    "for idx_col, target in enumerate(TARGET_VARIABLES):\n",
    "    actual = y_val_original.iloc[:, idx_col]\n",
    "    predicted = xgb_val_pred_original[:, idx_col]\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    xgb_metrics_original[target] = {\n",
    "        'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  {target}:\")\n",
    "    print(f\"    RMSE: AED {rmse:,.2f}\" if 'Revenue' in target or 'ADR' in target or 'RevPar' in target else f\"    RMSE: {rmse:.2f}%\")\n",
    "    print(f\"    MAE: AED {mae:,.2f}\" if 'Revenue' in target or 'ADR' in target or 'RevPar' in target else f\"    MAE: {mae:.2f}%\")\n",
    "    print(f\"    R²: {r2:.4f}\")\n",
    "    print(f\"    MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Date': val_df_clean['Date'].values,\n",
    "})\n",
    "\n",
    "for idx_col, target in enumerate(TARGET_VARIABLES):\n",
    "    comparison_df[f'{target}_Actual'] = y_val_original.iloc[:, idx_col].values\n",
    "    comparison_df[f'{target}_Predicted'] = xgb_val_pred_original[:, idx_col]\n",
    "    comparison_df[f'{target}_Error'] = comparison_df[f'{target}_Actual'] - comparison_df[f'{target}_Predicted']\n",
    "    comparison_df[f'{target}_Error_Pct'] = (comparison_df[f'{target}_Error'] / comparison_df[f'{target}_Actual']) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION COMPARISON SAMPLE (First 10 days)\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df[['Date', 'RevPar_Actual', 'RevPar_Predicted', 'RevPar_Error', 'RevPar_Error_Pct']].head(10))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION PERIOD SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for target in TARGET_VARIABLES:\n",
    "    actual_total = comparison_df[f'{target}_Actual'].sum()\n",
    "    predicted_total = comparison_df[f'{target}_Predicted'].sum()\n",
    "    diff = predicted_total - actual_total\n",
    "    diff_pct = (diff / actual_total) * 100\n",
    "    \n",
    "    print(f\"\\n{target}:\")\n",
    "    if 'Revenue' in target:\n",
    "        print(f\"  Actual Total: AED {actual_total:,.2f}\")\n",
    "        print(f\"  Predicted Total: AED {predicted_total:,.2f}\")\n",
    "        print(f\"  Difference: AED {diff:,.2f} ({diff_pct:+.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  Actual Average: {comparison_df[f'{target}_Actual'].mean():.2f}\")\n",
    "        print(f\"  Predicted Average: {comparison_df[f'{target}_Predicted'].mean():.2f}\")\n",
    "        print(f\"  Difference: {comparison_df[f'{target}_Predicted'].mean() - comparison_df[f'{target}_Actual'].mean():+.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Advanced Model: LSTM Neural Network\n",
    "\n",
    "Train LSTM for sequential time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ADVANCED MODEL: LSTM NEURAL NETWORK (MONTHLY)\")\nprint(\"=\"*80)\n\n# Prepare data for LSTM (reshape to 3D: samples, timesteps, features)\ndef create_sequences(X, y, timesteps=6):\n    Xs, ys = [], []\n    for i in range(len(X) - timesteps):\n        Xs.append(X.iloc[i:(i + timesteps)].values)\n        ys.append(y.iloc[i + timesteps].values)\n    return np.array(Xs), np.array(ys)\n\ntimesteps = 6  # Use 6 MONTHS of history (not 7 days!)\n\n# FIXED: Ensure all data is float32 for TensorFlow compatibility\nX_train_float32 = X_train.astype(np.float32)\ny_train_float32 = y_train.astype(np.float32)\nX_val_float32 = X_val.astype(np.float32)\ny_val_float32 = y_val.astype(np.float32)\n\nX_train_lstm, y_train_lstm = create_sequences(X_train_float32, y_train_float32, timesteps)\nX_val_lstm, y_val_lstm = create_sequences(X_val_float32, y_val_float32, timesteps)\n\nprint(f\"\\nLSTM Training data shape: {X_train_lstm.shape}\")\nprint(f\"LSTM Validation data shape: {X_val_lstm.shape}\")\n\n# Build LSTM model - MONTHLY optimized (reduced complexity)\nlstm_model = Sequential([\n    LSTM(24, activation='tanh', return_sequences=False,  # REDUCED from 32 for monthly\n         input_shape=(timesteps, X_train.shape[1])),\n    Dropout(0.3),\n    Dense(12, activation='relu'),  # REDUCED from 16 for monthly\n    Dense(len(TARGET_VARIABLES))  # Output layer for all targets\n])\n\n# Use lower learning rate to prevent overfitting\nlstm_model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n\nprint(\"\\nLSTM Model Architecture (Monthly-Optimized):\")\nlstm_model.summary()\n\n# Early stopping callback\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train LSTM with monthly-optimized settings\nprint(\"\\nTraining LSTM model with monthly-optimized complexity...\")\nhistory = lstm_model.fit(\n    X_train_lstm, y_train_lstm,\n    validation_data=(X_val_lstm, y_val_lstm),\n    epochs=50,\n    batch_size=16,\n    callbacks=[early_stop],\n    verbose=0\n)\nprint(\"✓ LSTM model trained\")\n\n# Predict on validation set\nlstm_val_pred = lstm_model.predict(X_val_lstm, verbose=0)\n\n# Calculate metrics\nprint(f\"\\nLSTM Validation Performance:\")\nlstm_metrics = {}\nfor idx, target in enumerate(TARGET_VARIABLES):\n    rmse = np.sqrt(mean_squared_error(y_val_lstm[:, idx], lstm_val_pred[:, idx]))\n    mae = mean_absolute_error(y_val_lstm[:, idx], lstm_val_pred[:, idx]))\n    r2 = r2_score(y_val_lstm[:, idx], lstm_val_pred[:, idx])\n    \n    lstm_metrics[target] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n    print(f\"\\n  {target}:\")\n    print(f\"    RMSE: {rmse:.2f}\")\n    print(f\"    MAE: {mae:.2f}\")\n    print(f\"    R²: {r2:.4f}\")\n\nresults['LSTM'] = lstm_metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training Loss Visualization (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LSTM training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('LSTM Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_title('LSTM Model MAE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('MAE', fontsize=11)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"MODEL COMPARISON - VALIDATION SET\")\nprint(\"=\"*80)\n\n# IMPORTANT NOTE: \n# - SARIMA metrics are in ORIGINAL SCALE (AED)\n# - XGBoost and LSTM metrics are in SCALED units\n# For fair comparison, we compare them separately or convert to same scale\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SARIMA Performance (Original Scale - AED)\")\nprint(\"=\"*80)\nprint(f\"\\nRevPar Prediction:\")\nprint(f\"  RMSE: AED {results['SARIMA']['RMSE']:.2f}\")\nprint(f\"  MAE: AED {results['SARIMA']['MAE']:.2f}\")\nprint(f\"  R²: {results['SARIMA']['R2']:.4f}\")\nprint(f\"  MAPE: {results['SARIMA']['MAPE']:.2f}%\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"XGBoost Performance (Original Scale - from Section 4.2.1)\")\nprint(\"=\"*80)\n# Use the metrics calculated in original scale from cell 53\nfor target in TARGET_VARIABLES:\n    if target in xgb_metrics_original:\n        print(f\"\\n{target}:\")\n        metrics = xgb_metrics_original[target]\n        if 'Revenue' in target or 'ADR' in target or 'RevPar' in target:\n            print(f\"  RMSE: AED {metrics['RMSE']:,.2f}\")\n            print(f\"  MAE: AED {metrics['MAE']:,.2f}\")\n        else:\n            print(f\"  RMSE: {metrics['RMSE']:.2f}%\")\n            print(f\"  MAE: {metrics['MAE']:.2f}%\")\n        print(f\"  R²: {metrics['R2']:.4f}\")\n        print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"LSTM Performance (Scaled Data)\")\nprint(\"=\"*80)\nprint(\"Note: LSTM metrics are in scaled units (not directly comparable to SARIMA)\")\nfor target in TARGET_VARIABLES:\n    print(f\"\\n{target}:\")\n    print(f\"  RMSE: {results['LSTM'][target]['RMSE']:.2f} (scaled)\")\n    print(f\"  MAE: {results['LSTM'][target]['MAE']:.2f} (scaled)\")\n    print(f\"  R²: {results['LSTM'][target]['R2']:.4f}\")\n\n# Create comparison visualization (RevPar only, original scale)\nprint(\"\\n\" + \"=\"*80)\nprint(\"VISUAL COMPARISON - RevPar (Original Scale)\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Compare SARIMA vs XGBoost (both in original scale)\nmodels_to_compare = ['SARIMA', 'XGBoost']\nmetrics_to_plot = ['RMSE', 'MAE', 'MAPE']\nmetric_values = {\n    'SARIMA': [\n        results['SARIMA']['RMSE'],\n        results['SARIMA']['MAE'],\n        results['SARIMA']['MAPE']\n    ],\n    'XGBoost': [\n        xgb_metrics_original['RevPar']['RMSE'],\n        xgb_metrics_original['RevPar']['MAE'],\n        xgb_metrics_original['RevPar']['MAPE']\n    ]\n}\n\nfor idx, metric in enumerate(metrics_to_plot):\n    values = [metric_values[model][idx] for model in models_to_compare]\n    axes[idx].bar(models_to_compare, values, color=['skyblue', 'orange'])\n    \n    if metric == 'MAPE':\n        axes[idx].set_title(f'{metric} Comparison - RevPar (%)', fontsize=14, fontweight='bold')\n        axes[idx].set_ylabel(f'{metric} (%)', fontsize=11)\n    else:\n        axes[idx].set_title(f'{metric} Comparison - RevPar (AED)', fontsize=14, fontweight='bold')\n        axes[idx].set_ylabel(f'{metric} (AED)', fontsize=11)\n    \n    axes[idx].grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels on bars\n    for i, v in enumerate(values):\n        axes[idx].text(i, v, f'{v:.2f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL SELECTION SUMMARY\")\nprint(\"=\"*80)\nprint(\"\\nKey Findings:\")\nprint(f\"  - SARIMA trained on ORIGINAL scale performs better for statistical modeling\")\nprint(f\"  - XGBoost trained on SCALED data benefits from regularization\")\nprint(f\"  - LSTM trained on SCALED data for neural network optimization\")\nprint(f\"\\nRecommendation: Use XGBoost for final forecast (multi-target, best R²)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON - VALIDATION SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "# SARIMA (RevPar only)\n",
    "comparison_data.append({\n",
    "    'Model': 'SARIMA',\n",
    "    'Target': 'RevPar',\n",
    "    'RMSE': results['SARIMA']['RMSE'],\n",
    "    'MAE': results['SARIMA']['MAE'],\n",
    "    'R²': results['SARIMA']['R2']\n",
    "})\n",
    "\n",
    "# XGBoost\n",
    "for target in TARGET_VARIABLES:\n",
    "    comparison_data.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Target': target,\n",
    "        'RMSE': results['XGBoost'][target]['RMSE'],\n",
    "        'MAE': results['XGBoost'][target]['MAE'],\n",
    "        'R²': results['XGBoost'][target]['R2']\n",
    "    })\n",
    "\n",
    "# LSTM\n",
    "for target in TARGET_VARIABLES:\n",
    "    comparison_data.append({\n",
    "        'Model': 'LSTM',\n",
    "        'Target': target,\n",
    "        'RMSE': results['LSTM'][target]['RMSE'],\n",
    "        'MAE': results['LSTM'][target]['MAE'],\n",
    "        'R²': results['LSTM'][target]['R2']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'R²']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    revpar_data = comparison_df[comparison_df['Target'] == 'RevPar']\n",
    "    axes[idx].bar(revpar_data['Model'], revpar_data[metric], \n",
    "                  color=['skyblue', 'orange', 'green'])\n",
    "    axes[idx].set_title(f'{metric} Comparison (RevPar)', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel(metric, fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Final Model Training (All Historical Data)\n",
    "\n",
    "Retrain the best model using all available data up to August 29, 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"FINAL MODEL TRAINING (TRAIN + VALIDATION)\")\nprint(\"=\"*80)\n\n# Combine train and validation for final training\nfinal_train_df = pd.concat([train_df_clean, val_df_clean]).reset_index(drop=True)\n\nX_final = final_train_df[feature_cols]\ny_final = final_train_df[TARGET_VARIABLES]\n\nprint(f\"\\nFinal training set: {X_final.shape[0]} records\")\nprint(f\"Date range: {final_train_df['Date'].min()} to {final_train_df['Date'].max()}\")\n\n# Retrain XGBoost with improved regularization parameters\nprint(\"\\nRetraining XGBoost on all historical data with regularization...\")\nfinal_xgb = MultiOutputRegressor(xgb.XGBRegressor(\n    n_estimators=200,\n    max_depth=5,  # Reduced from 6\n    learning_rate=0.05,  # Reduced from 0.1\n    subsample=0.8,\n    colsample_bytree=0.7,  # Reduced from 0.8\n    reg_alpha=0.1,  # L1 regularization\n    reg_lambda=1.0,  # L2 regularization\n    min_child_weight=3,  # Prevents overfitting\n    random_state=42,\n    n_jobs=-1\n))\n\nfinal_xgb.fit(X_final, y_final)\nprint(\"✓ Final XGBoost model trained with regularization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Forecasting the Future (Sept - Dec 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL FORECAST: SEPTEMBER - DECEMBER 2025\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare test data (handle NaN from lagging)\n",
    "# For forecasting, we need to handle missing lag values\n",
    "# Use forward fill or last known values\n",
    "test_df_forecast = test_df.copy()\n",
    "\n",
    "# Fill NaN with forward fill (use last known values)\n",
    "for col in feature_cols:\n",
    "    if test_df_forecast[col].isnull().any():\n",
    "        # Fill with the last value from validation set\n",
    "        last_val = val_df_clean[col].iloc[-1] if col in val_df_clean.columns else 0\n",
    "        test_df_forecast[col].fillna(last_val, inplace=True)\n",
    "\n",
    "X_test = test_df_forecast[feature_cols]\n",
    "\n",
    "print(f\"\\nTest set for forecasting: {X_test.shape[0]} records\")\n",
    "print(f\"Date range: {test_df_forecast['Date'].min()} to {test_df_forecast['Date'].max()}\")\n",
    "\n",
    "# Make predictions\n",
    "final_predictions = final_xgb.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "final_predictions_original = final_predictions.copy()\n",
    "for idx_col, target in enumerate(TARGET_VARIABLES):\n",
    "    if target in cols_to_scale:\n",
    "        target_idx = cols_to_scale.index(target)\n",
    "        dummy = np.zeros((len(final_predictions), len(cols_to_scale)))\n",
    "        dummy[:, target_idx] = final_predictions[:, idx_col]\n",
    "        inv_transformed = scaler.inverse_transform(dummy)\n",
    "        final_predictions_original[:, idx_col] = inv_transformed[:, target_idx]\n",
    "\n",
    "# Create forecast dataframe with original scale values\n",
    "forecast_df = pd.DataFrame(final_predictions_original, columns=TARGET_VARIABLES)\n",
    "\n",
    "# Create forecast dataframe\n",
    "forecast_df['Date'] = test_df_forecast['Date'].values\n",
    "\n",
    "print(\"\\n✓ Forecast complete!\")\n",
    "print(\"\\nForecast Summary (Sept - Dec 2025):\")\n",
    "display(forecast_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "source": "# DIAGNOSTIC: Check if forecast values are reasonable\nprint(\"\\n\" + \"=\"*80)\nprint(\"FORECAST DIAGNOSTICS\")\nprint(\"=\"*80)\n\nprint(f\"\\nForecast Statistics:\")\nprint(f\"  Revenue - Min: AED {forecast_df['Revenue'].min():,.2f}, Max: AED {forecast_df['Revenue'].max():,.2f}\")\nprint(f\"  RevPar - Min: AED {forecast_df['RevPar'].min():.2f}, Max: AED {forecast_df['RevPar'].max():.2f}\")\nprint(f\"  ADR - Min: AED {forecast_df['ADR'].min():.2f}, Max: AED {forecast_df['ADR'].max():.2f}\")\nprint(f\"  Occupancy - Min: {forecast_df['Occupancy_Pct'].min():.2f}%, Max: {forecast_df['Occupancy_Pct'].max():.2f}%\")\n\nprint(f\"\\nHistorical Comparison (2024 Sept-Dec for reference):\")\nhist_comp = df_clean[(df_clean['Date'] >= '2024-09-01') & (df_clean['Date'] <= '2024-12-31')]\nif len(hist_comp) > 0:\n    print(f\"  2024 Sept-Dec Revenue: AED {hist_comp['Revenue'].sum():,.2f}\")\n    print(f\"  2025 Forecast: AED {forecast_df['Revenue'].sum():,.2f}\")\n    diff_pct = ((forecast_df['Revenue'].sum() / hist_comp['Revenue'].sum()) - 1) * 100\n    print(f\"  Difference: {diff_pct:+.1f}%\")\nelse:\n    print(\"  No 2024 Sept-Dec data available for comparison\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Final Visualization and Total Revenue Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOTAL REVENUE PROJECTION (SEPT - DEC 2025)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate totals\n",
    "total_revenue = forecast_df['Revenue'].sum()\n",
    "avg_revpar = forecast_df['RevPar'].mean()\n",
    "avg_adr = forecast_df['ADR'].mean()\n",
    "avg_occupancy = forecast_df['Occupancy_Pct'].mean()\n",
    "\n",
    "print(f\"\\nProjected Totals (September 1 - December 31, 2025):\")\n",
    "print(f\"  Total Revenue: AED {total_revenue:,.2f}\")\n",
    "print(f\"  Average RevPar: AED {avg_revpar:.2f}\")\n",
    "print(f\"  Average ADR: AED {avg_adr:.2f}\")\n",
    "print(f\"  Average Occupancy: {avg_occupancy:.2f}%\")\n",
    "\n",
    "# Monthly breakdown\n",
    "forecast_df['Month'] = pd.to_datetime(forecast_df['Date']).dt.month\n",
    "monthly_summary = forecast_df.groupby('Month').agg({\n",
    "    'Revenue': 'sum',\n",
    "    'RevPar': 'mean',\n",
    "    'ADR': 'mean',\n",
    "    'Occupancy_Pct': 'mean'\n",
    "})\n",
    "\n",
    "print(\"\\nMonthly Breakdown:\")\n",
    "monthly_summary.index = ['September', 'October', 'November', 'December']\n",
    "display(monthly_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Full Year 2025 Summary (Actual + Forecasted)\n",
    "\n",
    "Combine actual data (Jan-Aug) with forecast (Sept-Dec) for complete year view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"FULL YEAR 2025 SUMMARY (ACTUAL + FORECASTED)\")\nprint(\"=\"*80)\n\n# FIXED: Get actual data for Jan-Aug 2025 and INVERSE TRANSFORM\nactual_2025_scaled = ml_ready_data[(ml_ready_data['Date'] >= '2025-01-01') &\n                                    (ml_ready_data['Date'] <= pd.to_datetime('2025-08-31'))].copy()\n\n# The data in ml_ready_data is SCALED, so we need to inverse transform it\n# Create a copy for inverse transformation\nactual_2025_original = actual_2025_scaled.copy()\n\n# Inverse transform each target variable\nfor target in TARGET_VARIABLES:\n    if target in cols_to_scale:\n        target_idx = cols_to_scale.index(target)\n        \n        # Get scaled values\n        scaled_values = actual_2025_scaled[target].values\n        \n        # Create dummy array for inverse transform\n        dummy = np.zeros((len(scaled_values), len(cols_to_scale)))\n        dummy[:, target_idx] = scaled_values\n        \n        # Inverse transform\n        inv_transformed = scaler.inverse_transform(dummy)\n        actual_2025_original[target] = inv_transformed[:, target_idx]\n\n# Now calculate metrics from ORIGINAL scale data\nactual_revenue_ytd = actual_2025_original['Revenue'].sum()\nactual_avg_revpar = actual_2025_original['RevPar'].mean()\nactual_avg_adr = actual_2025_original['ADR'].mean()\nactual_avg_occ = actual_2025_original['Occupancy_Pct'].mean()\n\n# Forecast totals (Sept-Dec)\nforecast_revenue = forecast_df['Revenue'].sum()\nforecast_avg_revpar = forecast_df['RevPar'].mean()\nforecast_avg_adr = forecast_df['ADR'].mean()\nforecast_avg_occ = forecast_df['Occupancy_Pct'].mean()\n\n# Full year totals\nfull_year_revenue = actual_revenue_ytd + forecast_revenue\nfull_year_avg_revpar = (actual_avg_revpar * len(actual_2025_original) + forecast_avg_revpar * len(forecast_df)) / (len(actual_2025_original) + len(forecast_df))\nfull_year_avg_adr = (actual_avg_adr * len(actual_2025_original) + forecast_avg_adr * len(forecast_df)) / (len(actual_2025_original) + len(forecast_df))\nfull_year_avg_occ = (actual_avg_occ * len(actual_2025_original) + forecast_avg_occ * len(forecast_df)) / (len(actual_2025_original) + len(forecast_df))\n\nprint(f\"\\n2025 Year-to-Date (Jan - Aug): ACTUAL\")\nprint(f\"  Months: {len(actual_2025_original)}\")\nprint(f\"  Total Revenue: AED {actual_revenue_ytd:,.2f}\")\nprint(f\"  Average RevPar: AED {actual_avg_revpar:,.2f}\")\nprint(f\"  Average ADR: AED {actual_avg_adr:,.2f}\")\nprint(f\"  Average Occupancy: {actual_avg_occ:.2f}%\")\n\nprint(f\"\\n2025 Forecast (Sept - Dec): FORECASTED\")\nprint(f\"  Months: {len(forecast_df)}\")\nprint(f\"  Total Revenue: AED {forecast_revenue:,.2f}\")\nprint(f\"  Average RevPar: AED {forecast_avg_revpar:,.2f}\")\nprint(f\"  Average ADR: AED {forecast_avg_adr:,.2f}\")\nprint(f\"  Average Occupancy: {forecast_avg_occ:.2f}%\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FULL YEAR 2025 TOTALS (Jan - Dec)\")\nprint(\"=\"*80)\nprint(f\"  Total Months: {len(actual_2025_original) + len(forecast_df)}\")\nprint(f\"  Total Revenue: AED {full_year_revenue:,.2f}\")\nprint(f\"  Average RevPar: AED {full_year_avg_revpar:,.2f}\")\nprint(f\"  Average ADR: AED {full_year_avg_adr:,.2f}\")\nprint(f\"  Average Occupancy: {full_year_avg_occ:.2f}%\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"BREAKDOWN BY PERIOD\")\nprint(\"=\"*80)\n\n# Create comparison table\nsummary_table = pd.DataFrame({\n    'Period': ['Jan-Aug (Actual)', 'Sept-Dec (Forecast)', 'Full Year 2025'],\n    'Months': [len(actual_2025_original), len(forecast_df), len(actual_2025_original) + len(forecast_df)],\n    'Total Revenue (AED)': [\n        f'{actual_revenue_ytd:,.2f}',\n        f'{forecast_revenue:,.2f}',\n        f'{full_year_revenue:,.2f}'\n    ],\n    'Avg RevPar (AED)': [\n        f'{actual_avg_revpar:,.2f}',\n        f'{forecast_avg_revpar:,.2f}',\n        f'{full_year_avg_revpar:,.2f}'\n    ],\n    'Avg ADR (AED)': [\n        f'{actual_avg_adr:,.2f}',\n        f'{forecast_avg_adr:,.2f}',\n        f'{full_year_avg_adr:,.2f}'\n    ],\n    'Avg Occupancy (%)': [\n        f'{actual_avg_occ:.2f}',\n        f'{forecast_avg_occ:.2f}',\n        f'{full_year_avg_occ:.2f}'\n    ]\n})\n\ndisplay(summary_table)\n\n# Monthly breakdown for full year\nprint(\"\\n\" + \"=\"*80)\nprint(\"MONTHLY BREAKDOWN - 2025\")\nprint(\"=\"*80)\n\n# Get monthly data for actual\nactual_2025_original['Month_Num'] = pd.to_datetime(actual_2025_original['Date']).dt.month\nactual_monthly = actual_2025_original.groupby('Month_Num').agg({\n    'Revenue': 'sum',\n    'RevPar': 'mean',\n    'ADR': 'mean',\n    'Occupancy_Pct': 'mean'\n})\n\n# Get monthly data for forecast\nforecast_df['Month_Num'] = pd.to_datetime(forecast_df['Date']).dt.month\nforecast_monthly = forecast_df.groupby('Month_Num').agg({\n    'Revenue': 'sum',\n    'RevPar': 'mean',\n    'ADR': 'mean',\n    'Occupancy_Pct': 'mean'\n})\n\n# Combine\nimport calendar\nmonthly_full = pd.concat([actual_monthly, forecast_monthly]).sort_index()\nmonthly_full.index = [calendar.month_name[i] for i in monthly_full.index]\n\n# Format for display\nmonthly_display = monthly_full.copy()\nmonthly_display['Type'] = ['Actual']*len(actual_monthly) + ['Forecast']*len(forecast_monthly)\nmonthly_display = monthly_display[['Type', 'Revenue', 'RevPar', 'ADR', 'Occupancy_Pct']]\nmonthly_display.columns = ['Type', 'Revenue (AED)', 'RevPar (AED)', 'ADR (AED)', 'Occupancy (%)']\n\ndisplay(monthly_display)\n\nprint(f\"\\n{'='*80}\")\nprint(f\"2025 ANNUAL REVENUE: AED {full_year_revenue:,.2f}\")\nprint(f\"{'='*80}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Historical + Forecast\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Combine historical and forecast data for plotting\n",
    "historical_data = ml_ready_data[ml_ready_data['Date'] <= VALIDATION_END].copy()\n",
    "\n",
    "for idx, var in enumerate(TARGET_VARIABLES):\n",
    "    # Plot historical data\n",
    "    axes[idx].plot(historical_data['Date'], historical_data[var], \n",
    "                   linewidth=1.5, color='blue', label='Historical (Actual)', alpha=0.7)\n",
    "    \n",
    "    # Plot forecast\n",
    "    axes[idx].plot(forecast_df['Date'], forecast_df[var], \n",
    "                   linewidth=2, color='red', label='Forecast (Sept-Dec 2025)', marker='o')\n",
    "    \n",
    "    # Add vertical lines\n",
    "    axes[idx].axvline(x=pd.to_datetime(TRAIN_END), color='green', \n",
    "                      linestyle='--', alpha=0.5, label='Train End')\n",
    "    axes[idx].axvline(x=pd.to_datetime(VALIDATION_END), color='orange', \n",
    "                      linestyle='--', alpha=0.5, label='Validation End')\n",
    "    \n",
    "    axes[idx].set_title(f'{var} - Historical vs Forecast', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date', fontsize=11)\n",
    "    axes[idx].set_ylabel(var, fontsize=11)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Model Diagnostics: Feature Importance (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (XGBOOST)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance for RevPar model (first target)\n",
    "revpar_model = final_xgb.estimators_[0]  # First estimator is for RevPar\n",
    "feature_importance = revpar_model.feature_importances_\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "top_n = 20\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(top_n), importance_df['Importance'].head(top_n), color='steelblue')\n",
    "plt.yticks(range(top_n), importance_df['Feature'].head(top_n))\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title(f'Top {top_n} Feature Importance (RevPar Prediction)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "display(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Residual Analysis (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals on validation set\n",
    "residuals = y_val.values - xgb_val_pred\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, target in enumerate(TARGET_VARIABLES):\n",
    "    axes[idx].scatter(val_df_clean['Date'].values, residuals[:, idx], \n",
    "                     alpha=0.6, color='purple')\n",
    "    axes[idx].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx].set_title(f'{target} - Residuals vs Time (Validation)', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date', fontsize=11)\n",
    "    axes[idx].set_ylabel('Residuals', fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Export Final Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export forecast to CSV\nforecast_output = 'hotel_revenue_forecast_sept_dec_2025.csv'\nforecast_df.to_csv(forecast_output, index=False)\n\nprint(\"=\"*80)\nprint(\"FORECAST EXPORT COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\\n✓ Forecast saved to: {forecast_output}\")\n\n# Download\nfiles.download(forecast_output)\nprint(\"\\n✓ File downloaded!\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PROJECT COMPLETE!\")\nprint(\"=\"*80)\nprint(f\"\\nFinal 2025 Revenue Projection (Sept-Dec): AED {total_revenue:,.2f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}